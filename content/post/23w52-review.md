---
title: "提示词工程的问题视角 2023 Week 52 回顾"
date: 2023-12-30 08:45:07+08:00
lastmod: 2023-12-30 08:45:07+08:00
draft: false
math: false
keywords: [回顾, 想法, 生活]
description: "提示词即沟通"
tags: [回顾, 记录, 生活]
categories: [生活]
author: "猛犸"
---

在刚刚过去的 12 月，OpenAI 发布了一份[提示词工程指南][1]。这份文档列出了一些和大语言模型助手沟通的基本原则，包括指令要清晰、复杂任务要拆分、最好给出示例、给出操作流程等等。

这些基本规则，和今年 4 月份吴恩达教授的短课程相似——那门课叫[《面向开发者的提示词工程》][2]。不过，在吴恩达教授的课程中，介绍的是用大模型开发应用的基本原则，以及大模型的几类常见应用：总结、判断、转换、扩展。容易看出，其中并没有包括人们常见的使用大模型的方式：**问答**。

人们希望大语言模型能直接给出答案。这是更为直觉的用法，或者说，人们希望能像使用搜索引擎那样使用大语言模型，或者像靠谱的人类助手相似，能够三言两语就完成沟通，而不是先做一堆准备工作，先写一篇小作文。

不过这可能难以在短期内实现。提示词工程（Prompt Engineering），目前还是有必要存在的。

![prompt][image-1]

捎带说明一下，本文主要讨论的是大语言模型中的提示词工程，因此文中没有使用“生成式人工智能”这个范围更大的术语，而全部都使用了“大语言模型”。

#### 什么是提示词工程

> 没想到答案，就不要寻找题目。

但凡日常生活中接触到、使用过一些大语言模型产品的人，应该对提示词工程都不陌生。提示词工程是指通过设计和优化输入提示，引导大语言模型更有效、更精确地生成所需的内容。

![prompt](https://1-1256632535.cos.ap-beijing.myqcloud.com/img/prompt.png)

[@宝玉xp][3] 设计了一段效果很好的[翻译提示词][4]：

> 你是一位精通简体中文的专业翻译，曾参与《纽约时报》和《经济学人》中文版的翻译工作，因此对于新闻和时事文章的翻译有深入的理解。我希望你能帮我将以下英文新闻段落翻译成中文，风格与上述杂志的中文版相似。
> 
> 规则：
> - 翻译时要准确传达新闻事实和背景。
> - 保留特定的英文术语或名字，并在其前后加上空格，例如："中 UN 文"。
> - 分成两次翻译，并且打印每一次结果：
> - 根据新闻内容直译，不要遗漏任何信息
> - 根据第一次直译的结果重新意译，遵守原意的前提下让内容更通俗易懂，符合中文表达习惯
> 
> 本条消息只需要回复 OK，接下来的消息我将会给你发送完整内容，收到后请按照上面的规则打印两次翻译结果。

这段提示词中包含了角色扮演、规则设定，以及让大语言模型自行回顾和改善。显然，这和人类日常对话的方式不同。这也就意味着，提示词工程是一种需要学习才能掌握的技能——虽然门槛不高，但是还是要学习的。

这和人们期盼的“使用 AI 应该像和人聊天”，显然差距不小。

有一种观点认为，随着 AI 大模型的能力越来越强，对真实世界的理解越来越多，人们将会越来越容易使用自然语言与 AI 沟通，提示词工程将不再有用武之地。2022 年 9 月 13 日，OpenAI 的 Sam Altman 和 LinkedIn 的 Reid Hoffman 有一场[对谈][5]。Sam Altman 认为提示词工程是个短期需求：

> “5 年后我们将不再需要提示词工程，或者只需在这方面做少量工作。将来的 AI 系统不会因为增补了某个特定词就会产生截然不同的输出，而是可以较好地理解自然语言，用户只需以文本和语音形式输入指令，即可让计算机完成图像生成、资料研究、心理咨询等复杂任务。”
> 
> “总的来说，用户只须使用自然语言就可以与计算机交互，当然，如果艺术家能想出更有创造性的描述，也自然就可以生成更好的图像。”

AI 大佬[杨立昆][6]也支持这种观点。他认为，之所以需要提示词工程，是因为现在的大语言模型的理解能力还比较弱。

![Yann LeCun][image-2]

![Yann LeCun][image-3]

而另一种观点则认为，考虑 AI 大模型的基本技术原理，提示词工程将会一直存在，而且将成为重要的技术，相当于大语言模型出现之前的编程技术一样。Sam Altman 似乎改变了自己的观点；2023 年 2 月 21 日，他发了一条 twitter：“写出好提示词是一项高杠杆技能。”

![Sam Altman][image-4]

这个观点和今年上半年的许多人相同。[不少企业][7]开始招聘“提示词工程师”，提示词工程课程、网站和插件纷纷涌现。虽然生成式人工智能的能力在不断提升，但它们仍然不能理解人类语言的真正含义。大语言模型的原理是统计和概率，而非真正意义上的理解。因此，即使能更好地处理自然语言，但在特定场景下生成高质量、精确的内容仍需精心设计的提示。

换言之，大语言模型学习了许多东西，也能做一些推理和判断，但是它不知道用户面临的具体情境。我们要使用大语言模型来解决问题，需要自己先整理好问题，把它精确描述出来——将存在于思想中、甚至自己意识不到的需求，转化为明确的文本请求。

这样可以解决两个问题：上下文和歧义。前者是因为问题背景的复杂，后者是因为人类语言的模糊。

真正的问题总是复杂的，而人们在日常沟通中已经习惯了双方共同默认的问题背景和情境——这就是为什么网上陌生人沟通往往会吵起来。而大语言模型并没有这些个人化的背景知识。复杂总需要在某个地方得到满足，或者是用户界面（想想拥有无数菜单和可选项的软件），或者是解决方案。ChatGPT 提供了一个简单的用户界面，于是复杂就转移到了解决方案上。结果就是，我们需要跟大模型明确说明它需要解决的问题。

除了这两个问题之外，还有一个更不容易解决的问题：幻觉。

#### 大模型是场梦

> 世事一场大梦，人生几度秋凉。

今年剑桥词典选出的年度词是 hallucinate。这是个动词，意思是“产生幻觉”，特指大语言模型一本正经地胡说八道。

![Cambridge Dictionary][image-5]

12 月初，OpenAI 的计算机科学家 [Andrej Karpathy][8] 写了篇[短文][9]，讨论了大语言模型的幻觉问题。他认为，本质上来看，大语言模型就是在做梦。幻觉不是大模型的问题，它是大模型的工作方式。或者说，它不是大语言模型的缺陷，而是大语言模型的特征。

![Andrej Karpathy][image-6]

It's not a Bug, it's a Feature!

![Andrej Karpathy][image-7]

如果把大语言模型视为光谱的一端，另一端则是搜索引擎。搜索引擎完全不做梦，它根据用户的输入内容精确查找，它没有幻觉，但也没有生成内容的能力。把大语言模型当成搜索引擎或者相反，都是错误的。

Karpathy 认为，当人们将大语言模型变成工作助手时，幻觉才成了需要解决的问题。人们需要引导大语言模型助手往正确的方向去做梦。也许检索增强生成（RAG, Retrieval-Augmented Generation）是个方向，但是它并非唯一方向。

#### 提示词工程视角的问题视角

> 问题其实就是你期望的东西和你体验的东西之间的差别。

所以我们可以看出，所谓提示词工程，就是在理解大语言模型基本原理的基础上，尝试设计一个文本序列，让大语言模型产生更好的结果的一系列活动。

其核心是准确描述问题，限制产生的回应。膜拜提示词工程、背下提示词模板，都没什么意义。提示词会随不同的大语言模型而变，但是规则始终如一。

![neon-god](https://1-1256632535.cos.ap-beijing.myqcloud.com/img/neon-god.png)

更普适的技能，其实是问题表述——即识别、分析和界定问题。这是人类自古钻研的领域，早就有了很好的教材：[《你的灯亮着吗？》][10]、 [《学会提问》][11]、 [《怎样解题》][12]，等等等等。

今年 6 月的《哈佛商业评论》网站上有一篇文章[《AI 提示词工程不是未来》][13]，作者是伦敦国王学院的[Oguz A. Acar][14]。他认为，在使用大语言模型时，应该从规划问题的角度来构建提示词。这包括几个步骤：问题识别、问题分解、问题重构、约束设计。

按照 [Gerald M. Weinberg][15] 的定义，问题是现状和理想之间的差距。描述理想中的目标，再描述现状，我们就得到了问题。有些问题简单具体，有些问题则宽泛得多。

对于宽泛的问题，就需要分解。澄清问题的本质和范围，将问题拆分成更小的可管理的子问题，尤其是当问题涉及多方面因素时更该如此。无论是“我怎样才能瘦 20 斤”还是“我怎样才能财务自由”，都不会有简单答案。

有时候，看待问题的角度决定了解决方案的数量和质量，《你的灯还亮着吗》第一章就提供了一个好例子。问题重构让我们在收集信息的过程中持续回顾问题，看看是否有其它看待问题的角度，甚至考虑最初问题的定义是否有偏差。

最后，我们要定义解决方案的输入、过程和输出来限制问题的边界，这会让大语言模型返回更合适的答案。提示词工程中的“Few-Shot Prompting”方法就是一种典型的约束设计方式，它通过举例来让大语言模型理解任务的要求。

没有明确定义的问题，即使是最复杂的提示词也无法得到优秀的结果。而若问题是定义清晰的，提示词在语言上的细微差别，可能并不那么重要。

我觉得，和大语言模型讨论并解决问题的过程和在论坛上求助有些相似，适用于技术论坛的提问规则也适用于它。所以，《提问的智慧》这篇已经有二十年历史的文章依然很有价值。这里是[英文版][16]，这里是翻译得很好的[中文版][17]。

一句话总结：尝试明确描述问题，一次把问题和想要的解决方案说清楚。

#### 更远的未来

今天的企业已经开始部署和使用自己的大语言模型，例如 [Amazon Web Services][18] 的 [Amazon Q][19]。企业操纵大语言模型的两种主要方法是微调和 RAG；微调改变了大语言模型的行为和响应，RAG 则通过可信的上下文来补充用户的输入。

也许几年后，我们也可以用类似的方式，把自己的私人大语言模型变成随身分身。也许将来算力、算法、真实世界理解和人机交互等等都会有重大突破，我们无需提示词工程也能获得满意的解决方案。

![sunset](https://1-1256632535.cos.ap-beijing.myqcloud.com/img/sunset.png)

但是基本规则不会变。2023 年 5 月 25 日，李彦宏在中关村论坛做了一场主题演讲：[《大模型将改变世界》][20]。他说，“10 年后，全世界有 50% 的工作会是提示词工程。”

时间会证明这句话是否正确。不过我觉得，他的下一句话才更关键：

“提出问题比解决问题更重要。”

---

本文图片除截图外，皆由 DALL·E 3 生成。

### 本周的成果

- 完成了基本工作
- 练习使用几个工具

### 本周的改变

- 变懒了
- 精力好像变差了

### 做得还不错

- 基本上没有头痛

### 做得不太好

- 作息还是比较混乱
- 有不少可选工作没有完成
- 花了不少时间看网络小说

### 下周的目标

- 继续努力早睡早起
- 做新年计划
- 推进几个长期工作

[1]:	https://platform.openai.com/docs/guides/prompt-engineering
[2]:	https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/
[3]:	https://weibo.com/u/1727858283
[4]:	https://weibo.com/1727858283/NlsDSpPaa
[5]:	https://www.youtube.com/watch?v=WHoWGNQRXb0&t=1523s
[6]:	https://en.wikipedia.org/wiki/Yann_LeCun
[7]:	https://www.linkedin.com/jobs/prompt-engineer-jobs/?currentJobId=3690188198
[8]:	https://karpathy.ai/
[9]:	https://twitter.com/karpathy/status/1733299213503787018
[10]:	https://book.douban.com/subject/25772550/
[11]:	https://book.douban.com/subject/20428922/
[12]:	https://book.douban.com/subject/34900781/
[13]:	https://hbr.org/2023/06/ai-prompt-engineering-isnt-the-future
[14]:	https://www.kcl.ac.uk/people/oguz-a-acar
[15]:	https://en.wikipedia.org/wiki/Gerald_Weinberg
[16]:	http://www.catb.org/~esr/faqs/smart-questions.html
[17]:	https://github.com/ryanhanwu/How-To-Ask-Questions-The-Smart-Way/blob/main/README-zh_CN.md
[18]:	https://aws.amazon.com/
[19]:	https://aws.amazon.com/cn/blogs/aws/introducing-amazon-q-a-new-generative-ai-powered-assistant-preview/
[20]:	https://new.qq.com/rain/a/20230527A07Q1800

[image-1]:	https://1-1256632535.cos.ap-beijing.myqcloud.com/img/CleanShot%202023-12-17%20at%2020.05.25@2x.png
[image-2]:	https://1-1256632535.cos.ap-beijing.myqcloud.com/img/CleanShot%202023-12-17%20at%2016.06.40@2x.png
[image-3]:	https://1-1256632535.cos.ap-beijing.myqcloud.com/img/CleanShot%202023-12-17%20at%2016.08.09@2x.png
[image-4]:	https://1-1256632535.cos.ap-beijing.myqcloud.com/img/CleanShot%202023-12-17%20at%2016.04.33@2x.png
[image-5]:	https://1-1256632535.cos.ap-beijing.myqcloud.com/img/CleanShot%202023-12-31%20at%2014.20.13@2x.png
[image-6]:	https://1-1256632535.cos.ap-beijing.myqcloud.com/img/CleanShot%202023-12-17%20at%2014.30.43@2x.png
[image-7]:	https://1-1256632535.cos.ap-beijing.myqcloud.com/img/CleanShot%202023-12-17%20at%2014.31.38@2x.png
