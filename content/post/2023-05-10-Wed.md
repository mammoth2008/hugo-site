---
title: "日贴230510 Wed Deep Learning 1.2"
date: 2023-05-10 18:25:09+08:00
lastmod: 2023-05-10 18:25:09+08:00
draft: false
math: true
keywords: [AI, 读书, 生活]
description: "日日用功"
tags: [深度学习,  AI, 笔记]
categories: [笔记]
author: "猛犸"
---

## Ch1 1.2 深度学习的历史趋势

一般认为，深度学习到目前已经经历了三次发展浪潮：20 世纪 40 年代到 60 年代的控制论(Cybernetics)，20 世纪 80 年代到 90 年代的连接主义(connectionism)和 2006 年以来的深度学习。

现代术语"深度学习"超越了目前机器学习模型的神经科学观点。它述诸于学习多层次组合这一更普遍的原理。

现代深度学习最早的前身是从神经科学角度出发的简单线性模型。这些模型设计为使用一组 $ n $ 个输入 $ x\_1,\dots,x\_n$ 来预测一个输出 $ y $。这些模型希望学习一组权重 $ w\_1,\dots,w\_n $，并计算它们的输出 $ f(\mathbf{x,w} )=x\_1w\_1+\dots+x\_nw\_n $。

McCulloch-Pitts 神经元是脑功能的早期模型。感知机成为第一个能根据每个类别的输入样本来学习权重的模型。自适应线性单元(ADALINE)简单地返回函数 $ f(\mathbf{x,w}) $ 的值来预测一个实数。用于调节 ADALINE 权重的学习算法被称为随机梯度下降(SGD, Stochasitc Gradient Descent)。 基于感知机和 ADALINE 的模型被称为线性模型。线性模型有许多局限性，最著名的是，它们不能学习异或函数(XOR)。

在 20 世纪 80 年代，神经网络的研究者们开始研究如何训练多层神经网络。这些网络被称为多层感知机(MLP, Multilayer Perceptron)。MLP 由许多神经元组成，每个神经元都是一个线性模型，但是使用非线性函数来激活。这些非线性函数被称为激活函数(activation function)。MLP 可以学习异或函数，但是在 20 世纪 90 年代，研究者们发现，训练 MLP 是困难的。这些困难被称为梯度消失问题(gradient vanishing problem)。在 20 世纪 90 年代，研究者们开始使用其他模型，如支持向量机(SVM, Support Vector Machine)和决策树(decision tree)。

20 世纪 80 年代，神经网络研究的第二次浪潮在很大程度上是伴随一个被称为联结主义(connectionism)或并行分布处理(parallel distributed processing)潮流而出现的，它的背景是认知科学。联结主义的中心思想是，当网络将大量简单的计算单元连接在一起时，可以实现智能行为。

在 20 世纪 80 年代的联结主义期间，形成的几个关键概念，在今天的深度学习中仍然是非常重要的：

- 分布式表示(distributed representation): 一个概念可以由许多不同的特征来表示，而不是由一个特定的特征来表示。例如，一个人可以被表示为年龄、性别、国籍、职业等特征的集合。这些特征可以以不同的方式组合来表示不同的人。这种分布式表示的好处是，它可以更容易地泛化到新的情况，并且可以更有效地利用统计信息。
- 反向传播算法(backpropagation): 一种有效的训练多层神经网络的方法。它可以被看作是链式法则的应用，用于计算神经网络中每个参数的梯度。
- 长短时记忆(LSTM, Long Short-Term Memory): 一种特殊的神经网络结构，可以有效地学习长期依赖关系。它是一种递归神经网络(RNN, Recurrent Neural Network)，它的参数可以在时间上共享。

神经网络的第三次浪潮始于 2006 年的突破。Geoffrey Hinton 表明名为"深度信念网络"的神经网络可以使用一种称为"贪婪逐层预训练"的策略来有效训练。这种策略是，首先训练一个模型的第一层，然后将其输出作为第二层的训练数据，以此类推。这种策略的好处是，它可以避免梯度消失问题，并且可以更有效地训练深度模型。这种策略的缺点是，它不能直接应用于所有类型的模型，并且它需要一个预训练阶段，这增加了训练时间。

在 2006 年之后，研究者们发现，使用更好的优化算法，如 AdaGrad， RMSProp 和 Adam，可以更有效地训练深度模型。这些算法的一个关键特性是，它们可以自动调整每个参数的学习率。这些算法的另一个关键特性是，它们可以使用更复杂的模型，如卷积神经网络(CNN, Convolutional Neural Network)和递归神经网络(RNN, Recurrent Neural Network)，而不需要预训练。

在 2012 年，Alex Krizhevsky 等人使用卷积神经网络(CNN)赢得了 ImageNet 图像分类挑战赛。这是深度学习的一个重要里程碑，因为它表明，深度学习可以在实际应用中取得出色的结果。

随着训练数据的增加，所需的技巧正在减少。截止 2016 年，一个简单的经验法则是：监督深度学习算法在每类给定 5000 个标记样本的情况下，可以很好地工作；当至少有 1000 万个标注样本的数据集用于训练时，它将达到或超过人类表现。

深度学习的另一个重大成就，是在强化学习(reinforcement learning)中的扩展。在强化学习中，一个自主的智能体必须在没有人类操作者指导的情况下，通过试错来学习执行任务。

[《深度学习》](https://book.douban.com/subject/27087503/)

原作名: Deep Learning: Adaptive Computation and Machine Learning series

作者: [美] 伊恩·古德费洛 / [加] 约书亚·本吉奥 / [加] 亚伦·库维尔

译者: 赵申剑 / 黎彧君 / 符天凡 / 李凯

出版年: 2017-7-1

出版社: 人民邮电出版社
